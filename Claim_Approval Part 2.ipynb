{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insurance Claim dataset: Model Creation\n",
    "***\n",
    "- In this notebook (second of two parts), we build a predictive model on the insurance claim data. \n",
    "- We train several models: Logistic Regression, Random Forest and Artifical Neural network. \n",
    "- We test the models on some holdout data for accuracy and other evaluation metrics.\n",
    "- We save these models to the disk for later use.\n",
    "- We also show how a saved and trained models could be loaded and used with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import model_from_json\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from fancyimpute import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,x21,x22,x23,x24,x25,x26,x27,x28,x29,x30,x31,x32,x33,x36,x37,x38,x39,x40,x41,x42,x43,x44,x45,x46,x47,x48,x49,x50,x51,x52,x53,x54,x55,x56,x57,x58,x59,x60,x61,x62,x63,x64,x65,x66,x67,x69,x70,x71,x72,x73,x74,x75,x76,x77,x78,x79,x80,x81,x82,x83,x84,x85,x86,x87,x88,x89,x90,x91,x92,x94,x95,x96,x97,x98,x99,brand_bmw,brand_chevrolet,brand_chrystler,brand_ford,brand_honda,brand_mercades,brand_nissan,brand_tesla,brand_toyota,brand_volkswagon,day_fri,day_mon,day_thur,day_tues,day_wed,month_apr,month_aug,month_dec,month_feb,month_jan,month_july,month_jun,month_mar,month_may,month_nov,month_oct,month_sept,country_america,country_asia,country_euorpe,y\r\n",
      "0.198559608853,74.425319549,67.6277449069,-3.09511140802,-6.82232689348,19.0480707858,-0.362378194917,-10.6991738418,-22.6997911893,-1.56126206373,1.14861768226,-3.04335060595,-3.89655764707,2.17061450755,6.36629833434,-7.34086634511,0.267735423569,1.08131144748,-15.2368850395,1.15930626855,-27.9616816069,11.8290132779,43.2955834526,-0.880306671065,-4.51868785502,3.27109060903,0.103514430612,31.6468939601,2.90184868172,-0.163627371131,-2.13199383826,-8.46072778757,-2.41149951006,-7.35011007576,1.42359036999,-10.8392002374,-3.36651609252,15.494000211,4.55051837293,449.48,-44.0191379783,0.890008202017,-104.388382301,0.0,-11.3418646772,0.453917314812,2.80011116379,5.69180559031,35.7825099759,-22.5674290132,-0.199236863364,0.871390580404,5.80816211865,-1.46789664513,34.0301835262,56.1257482514,2.07839632777,-0.307610213931,4.62910259742,14.791114654,-4.0873230989,4.03642608813,-1.5338611405,1.26170679401,59.9354634295,0.643822539595,-31.391164693,41.0402059347,8.61649590966,9.52908225308,-18.0018859549,-5.77564295044,40.6171068443,1.96569467175,-50.6631412143,4.55405529175,-7.49583259842,-46.6907443007,6.48153837723,4.63583146281,1.88514871963,0.784681229821,0.556071052997,-0.103677321858,11.068765404,-1.40518827709,-1.96028006013,-155.442236871,0.800947539908,1.55384551669,-1.09392573826,16.2025570045,26.238590723,-2.12556999938,9.64446607337,1.23766729714,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 ./datasets/train2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"./datasets/train2.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39965, 127)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39965, 127)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The whole data_matrix\n",
    "data_mat = df_train.values\n",
    "data_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for train and dev/test purpose \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_mat[:,0:126], data_mat[:,126],test_size=0.10, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35968, 126)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2099.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_train[1:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalizer or Standardized\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "Xn_train = scaler.transform(X_train)\n",
    "Xn_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.2912184138\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression: Look at the parameters list\n",
    "### Try different parameters\n",
    "log_reg = LogisticRegression(tol=0.0000001, C=10, max_iter=1000000)\n",
    "log_reg.fit(Xn_train, Y_train)\n",
    "Y_pred = log_reg.predict(Xn_test)\n",
    "accuracy = (1 - np.sum(np.abs(Y_pred-Y_test))/Y_test.shape[0])*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "\n",
    "#Parameter space\n",
    "parameters = {'C':[0.01, 0.1, 1, 10]}\n",
    "#Estimator\n",
    "lr = LogisticRegression(tol=0.0001, max_iter=10000)\n",
    "# Method of Search, scoring, and Cross-validation strategy\n",
    "clf = GridSearchCV(lr, parameters, cv = 5, scoring=None)\n",
    "\n",
    "# Train the estimator through all the point on the grid\n",
    "clf.fit(Xn_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8912366548042705"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=40, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=111, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randF_clf = RandomForestClassifier(n_estimators=100, random_state=111, max_features=40)\n",
    "randF_clf.fit(Xn_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034275706780085"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randF_clf.score(Xn_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      1.00      0.94      3172\n",
      "        1.0       0.98      0.54      0.70       825\n",
      "\n",
      "avg / total       0.91      0.90      0.89      3997\n",
      "\n",
      "('Accuracy: ', 90.342757067800846)\n"
     ]
    }
   ],
   "source": [
    "Y_pred = randF_clf.predict(Xn_test)\n",
    "print classification_report(Y_test, Y_pred)\n",
    "accuracy = (1 - np.sum(np.abs(Y_pred-Y_test))/Y_test.shape[0])*100\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network using Keras\n",
    "***\n",
    "We create a neural networks with three hidden layers having 32, 16, 8 nodes and 1 output node. We implement dropouts in each of these layers for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network using Keras\n",
    "def model_ann(input_nodes):\n",
    "    '''\n",
    "    This function creates artifical neural network with fixed number of layers and nodes.\n",
    "    ---\n",
    "    Parameters:\n",
    "    ----\n",
    "    input_nodes: Number of input nodes in the network\n",
    "    ----\n",
    "    Returns: A sequential neural network model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Hidden Layer\n",
    "    model.add(Dense(32, input_dim=input_nodes, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Second Hidden Layer\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Third Hidden Layer\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                4064      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_ann = model_ann(Xn_train.shape[1])\n",
    "print model_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "35968/35968 [==============================] - 2s - loss: 0.5027 - acc: 0.7693     \n",
      "Epoch 2/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.3252 - acc: 0.8675     \n",
      "Epoch 3/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.2641 - acc: 0.9022     \n",
      "Epoch 4/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.2180 - acc: 0.9237     \n",
      "Epoch 5/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1857 - acc: 0.9383     \n",
      "Epoch 6/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1623 - acc: 0.9487     \n",
      "Epoch 7/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1427 - acc: 0.9586     \n",
      "Epoch 8/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1286 - acc: 0.9641     \n",
      "Epoch 9/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1206 - acc: 0.9679     \n",
      "Epoch 10/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1153 - acc: 0.9693     \n",
      "Epoch 11/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1084 - acc: 0.9720     \n",
      "Epoch 12/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1082 - acc: 0.9722     \n",
      "Epoch 13/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1009 - acc: 0.9748     \n",
      "Epoch 14/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.1011 - acc: 0.9742     \n",
      "Epoch 15/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0915 - acc: 0.9771     \n",
      "Epoch 16/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0964 - acc: 0.9768     \n",
      "Epoch 17/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0906 - acc: 0.9780     \n",
      "Epoch 18/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0912 - acc: 0.9790     \n",
      "Epoch 19/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0895 - acc: 0.9782     \n",
      "Epoch 20/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0860 - acc: 0.9789     \n",
      "Epoch 21/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0858 - acc: 0.9785     \n",
      "Epoch 22/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0819 - acc: 0.9799     \n",
      "Epoch 23/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0859 - acc: 0.9795     \n",
      "Epoch 24/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0820 - acc: 0.9801     \n",
      "Epoch 25/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0817 - acc: 0.9802     \n",
      "Epoch 26/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0806 - acc: 0.9807     \n",
      "Epoch 27/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0813 - acc: 0.9808     \n",
      "Epoch 28/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0809 - acc: 0.9807     \n",
      "Epoch 29/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0805 - acc: 0.9810     \n",
      "Epoch 30/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0798 - acc: 0.9806     \n",
      "Epoch 31/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0790 - acc: 0.9808     \n",
      "Epoch 32/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0829 - acc: 0.9797     \n",
      "Epoch 33/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0787 - acc: 0.9815     \n",
      "Epoch 34/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0753 - acc: 0.9815     \n",
      "Epoch 35/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0729 - acc: 0.9828     \n",
      "Epoch 36/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0792 - acc: 0.9803     \n",
      "Epoch 37/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0803 - acc: 0.9808     \n",
      "Epoch 38/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0758 - acc: 0.9807     \n",
      "Epoch 39/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0771 - acc: 0.9818     \n",
      "Epoch 40/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0742 - acc: 0.9820     \n",
      "Epoch 41/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0756 - acc: 0.9825     \n",
      "Epoch 42/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0749 - acc: 0.9815     \n",
      "Epoch 43/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0748 - acc: 0.9824     \n",
      "Epoch 44/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0750 - acc: 0.9822     \n",
      "Epoch 45/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0741 - acc: 0.9832     \n",
      "Epoch 46/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0790 - acc: 0.9818     \n",
      "Epoch 47/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0750 - acc: 0.9816     \n",
      "Epoch 48/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0764 - acc: 0.9814     \n",
      "Epoch 49/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0761 - acc: 0.9827     \n",
      "Epoch 50/50\n",
      "35968/35968 [==============================] - 1s - loss: 0.0767 - acc: 0.9817     \n",
      "1664/3997 [===========>..................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model_ann.fit(Xn_train, Y_train,\n",
    "          epochs=50,\n",
    "          batch_size=128)\n",
    "score = model_ann.evaluate(Xn_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  [0.072297399163186504, 0.98423817863397545]\n",
      "3872/3997 [============================>.] - ETA: 0s\n",
      " The Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99      3172\n",
      "        1.0       0.98      0.95      0.96       825\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy: \", score\n",
    "\n",
    "Y_pred = model_ann.predict_classes(Xn_test)\n",
    "\n",
    "print \"\\n The Classification Report: \\n\", classification_report(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the scaler\n",
    "pickle.dump(scaler, open(\"./datasets/Scaler.pkl\", 'wb'))\n",
    "\n",
    "\n",
    "# SAVE PCA on disk\n",
    "#pickle.dump(skl_pca, open(\"./data/SKL_PCA_120517.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE SVM MODEL\n",
    "pickle.dump(randF_clf, open(\"./datasets/RandomForest1.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON and save to disk\n",
    "model_ann_json = model_ann.to_json()\n",
    "with open(\"./datasets/keras_mode.json\", \"wb\") as json_fl:\n",
    "    json_fl.write(model_ann_json)\n",
    "    \n",
    "# Also serialize weights to HDF5 and save\n",
    "model_ann.save_weights(\"./datasets/keras_model_weights.h5\")\n",
    "print(\"Successfully saved the model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability prediction after Loding the model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load test data taht has no labels\n",
    "df_test = pd.read_csv(\"./datasets/test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 126)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalizer or Standardized or Binarization\n",
    "scaler_loaded = pickle.load(open(\"./datasets/Scaler.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the test data\n",
    "Xn_test = scaler_loaded.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load trained RandomForestClassifier\n",
    "randF_loaded = pickle.load(open(\"./datasets/RandomForest1.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.9034275706780085)\n"
     ]
    }
   ],
   "source": [
    "#Check that loaded model is working fine\n",
    "Y_pred2 = randF_loaded.predict(Xn_test)\n",
    "print(\"Accuracy: \", randF_loaded.score(Xn_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# LOAD ANN: json and create model\n",
    "with open('./datasets/keras_mode.json', 'rb') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "loaded_model_ann = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model_ann.load_weights(\"./datasets/keras_model_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.32%\n"
     ]
    }
   ],
   "source": [
    " # MAKE SURE that loaded ANN is working fine\n",
    "loaded_model_ann.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "score = loaded_model_ann.evaluate(Xn_test, Y_test, verbose=0, batch_size=128)\n",
    "print(\"Accuracy: %.2f%%\" % ( score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
